Ciencia, Matemáticas, Coches sin conductor, Algoritmos computacionales, Tecnología, Vehículos

Las matemáticas de las máquinas morales

La adopción masiva de los coches sin conductor plantea una nueva serie de retos, no solo tecnológicos sino también de carácter ético

Se espera que, en las próximas décadas, los vehículos autónomos revolucionen el transporte de personas y bienes, mejorando la seguridad, reduciendo emisiones, incrementando el confort de los pasajeros y aumentando las posibilidades de transporte de personas con movilidad reducida. Pero la adopción masiva de estos vehículos por parte de la sociedad también plantea una nueva serie de retos, no solo tecnológicos sino también de carácter ético. Las matemáticas, especialmente la rama del análisis de riesgos, permiten afrontar de forma transparente estos debates morales, aportando un marco de apoyo a la toma de decisiones éticas en vehículos autónomos.
Siendo honestos, aún estamos lejos de poder tomar de forma rutinaria un taxi sin conductor y conversar con su sistema de infotenimiento. De momento, los vehículos autónomos (ADS, por sus siglas en inglés) más prometedores en el medio plazo son los denominados de nivel tres y cuatro en los que, aunque la conducción sea autónoma, se puede requerir intervención humana para su conducción en determinadas circunstancias. Pero, ¿cuándo ha de solicitarse la intervención? Si el vehículo predice una situación peligrosa, se espera que el conductor esté alerta y dispuesto a retomar el control; sin embargo, si está distraído y el coche transfiere el control, las consecuencias podrían ser catastróficas. ¿Qué debería hacer el vehículo? Y, ¿quién sería responsable de una eventual catástrofe? Esta cuestión, que se conoce como dilema fundamental de los ADS de nivel tres y cuatro, ha sido recientemente planteado en el marco del proyecto europeo Trustonomy, que busca aumentar la confianza del gran público en estas tecnologías y, en particular, abordar diversos debates éticos alrededor de las mismas.
Tratar de dar una respuesta a cuestiones éticas como el anterior dilema fundamental fue uno de los objetivos del proyecto Moral Machine, que enfrentó a millones de individuos a este tipo de situaciones para obtener información sobre las prioridades éticas colectivas en diferentes culturas. Entre sus conclusiones destaca la marcada diferencia de respuesta entre culturas colectivistas (como China o Japón) e individualistas (como Francia o EEUU).
Por tanto, no existe un consenso ético universal en lo que respecta a las decisiones de emergencia en conducción autónoma y no es posible dar respuestas únicas y universales a tales dilemas. Sin embargo, sí se pueden modelizar los diferentes sistemas éticos a través de las matemáticas, para guiar de forma coherente la toma de decisiones de un ADS. Como afirma el catedrático emérito de la Universidad Duke, Ralph Keeney, cualquier corriente ética puede modelizarse con ayuda del análisis de riesgos, desde la deontológica a la consecuencialista, incluyendo aproximaciones tanto utilitaristas como autoprotectivas.
Recientemente, dentro del proyecto Trustonomy, se ha desarrollado un nuevo modelo para la toma de decisiones en ADS que tiene en cuenta objetivos múltiples: rendimiento del vehículo, confort de los pasajeros, duración del viaje, seguridad –la de los pasajeros, las personas en la escena de conducción, el propio vehículo y la infraestructura– e incluso la reputación del fabricante. Una vez definidos los objetivos, cada productor podría decidir ponderarlos de distinta manera, dando más importancia a aquellos que sean de su interés; por ejemplo, se podría dar mayor relevancia a la seguridad de los pasajeros, a costa de una menor de los peatones. Para ello, se emplean pesos que permiten combinar los distintos objetivos en una única función, llamada función de utilidad multiatributo. Esta función regiría el funcionamiento del vehículo.
El modelo planteado logra estandarizar y hacer transparente el proceso de toma de decisiones en los ADS. Esto permite, entre otras cosas, reproducir esta toma de decisiones en entornos simulados. Gracias a ello, las entidades reguladoras competentes podrían simular múltiples escenas de conducción y fijar estándares de uso. Además, en caso de que un ADS sufra un accidente, podría simularse su operativa utilizando la función de utilidad que guiaba las decisiones del vehículo cuando ocurrió el accidente. Estas simulaciones permitirían evaluar si el vehículo satisfacía la regulación vigente y, en caso de no hacerlo, fijar responsabilidades.
Determinar la toma de decisiones de un ADS es un problema muy complejo que se refiere no solo a las situaciones de emergencia. Esta complejidad viene bien reflejada en el reciente informe de la Comisión Europea Ethics of connected and automated vehicles donde se identifican recomendaciones en relación con la seguridad y el riesgo en las carreteras, los aspectos éticos de los datos y los algoritmos y la cuestión de la responsabilidad. Sin duda, esto plantea infinidad de retos tecnológicos, en los que las matemáticas desempeñarán un papel central.
David Ríos es profesor de investigación del Consejo Superior de Investigaciones Científicas (CSIC) y AXA-ICMAT Chair en el ICMAT y miembro numerario de la Real Academia de Ciencias Exactas, Físicas y Naturales de España.
Roi Naveiro es investigador postdoctoral del CSIC en el ICMAT
Café y Teoremas es una sección dedicada a las matemáticas y al entorno en el que se crean, coordinado por el Instituto de Ciencias Matemáticas (ICMAT), en la que los investigadores y miembros del centro describen los últimos avances de esta disciplina, comparten puntos de encuentro entre las matemáticas y otras expresiones sociales y culturales y recuerdan a quienes marcaron su desarrollo y supieron transformar café en teoremas. El nombre evoca la definición del matemático húngaro Alfred Rényi: “Un matemático es una máquina que transforma café en teoremas”.
Edición y coordinación: Ágata A. Timón G Longoria (ICMAT).
Puedes seguir a MATERIA en Facebook, Twitter e Instagram, o apuntarte aquí para recibir nuestra newsletter semanal.
