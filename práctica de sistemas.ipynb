{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f95ec6e",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li>El Mundo (https://elmundo.es) </li>\n",
    "    <ul> \n",
    "        <li>Salud: https://www.elmundo.es/ciencia-y-salud/salud.html </li>\n",
    "        <li>Tecnología: https://www.elmundo.es/tecnologia.html  </li>\n",
    "        <li>Ciencia: https://www.elmundo.es/ciencia-y-salud/ciencia.html</li>\n",
    "    </ul>\n",
    "  <li>El País (https://elpais.com/)</li>\n",
    "    <ul> \n",
    "        <li>Sanidad: https://elpais.com/noticias/sanidad/ </li>\n",
    "        <li>Tecnología: https://elpais.com/tecnologia/</li>\n",
    "        <li> Ciencia: https://elpais.com/ciencia/</li>\n",
    "    </ul>\n",
    "  <li>20 Minutos (https://www.20minutos.es/) </li>\n",
    "    <ul> \n",
    "        <li>Salud: https://www.20minutos.es/salud/ </li>\n",
    "        <li>Tecnología: https://www.20minutos.es/tecnologia/ </li>\n",
    "        <li>Ciencia: https://www.20minutos.es/ciencia/ </li>\n",
    "    </ul>\n",
    " \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7f3858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a74057f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File tecnologia.2021-12-06.001.txt created!\n",
      "File tecnologia.2021-12-07.001.txt created!\n",
      "File tecnologia.2021-12-07.002.txt created!\n",
      "File tecnologia.2021-12-07.003.txt created!\n",
      "File tecnologia.2021-12-06.002.txt created!\n",
      "File tecnologia.2021-12-04.001.txt created!\n",
      "File tecnologia.2021-12-05.001.txt created!\n",
      "File tecnologia.2021-12-05.002.txt created!\n",
      "File tecnologia.2021-12-04.002.txt created!\n",
      "File tecnologia.2021-12-04.003.txt created!\n",
      "File tecnologia.2021-12-05.003.txt created!\n",
      "File tecnologia.2021-12-03.001.txt created!\n",
      "File tecnologia.2021-12-03.002.txt created!\n",
      "File tecnologia.2021-12-03.003.txt created!\n",
      "File tecnologia.2021-12-03.004.txt created!\n",
      "File tecnologia.2021-12-01.001.txt created!\n",
      "File tecnologia.2021-11-29.001.txt created!\n",
      "File tecnologia.2021-11-26.001.txt created!\n",
      "File tecnologia.2021-11-24.001.txt created!\n",
      "File tecnologia.2021-10-25.001.txt created!\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/con-proposito/2021-12-01/asi-es-la-luz-del-invierno-en-los-rincones-mas-bonitos.html\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/con-proposito/2021-11-24/cuatro-televisores-y-una-barra-de-sonido-con-los-que-triunfar-estas-navidades.html\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/5g-el-futuro-es-ahora/2021-11-20/el-5g-lleva-los-videojuegos-a-la-siguiente-pantalla.html\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/5g-el-futuro-es-ahora/2021-10-29/ciberseguridad-y-5g-una-tecnologia-mas-segura-en-un-entorno-mas-complejo.html\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/radar-pyme/2021-11-29/plasticos-del-mar-para-fabricar-muebles-y-coches-movidos-por-hidrogeno-algunas-ideas-para-descarbonizar-nuestra-vida.html\n",
      "Error procesando articulo, saltando el articulo https://elpais.com/tecnologia/radar-pyme/2021-11-04/objetivo-salvar-el-planeta-sin-dejar-atras-a-las-personas.html\n",
      "File tecnologia.2021-12-03.005.txt created!\n",
      "File tecnologia.2021-11-30.001.txt created!\n",
      "File tecnologia.2021-11-16.001.txt created!\n"
     ]
    }
   ],
   "source": [
    "# El Pais\n",
    "\n",
    "diario = \"https://elpais.com\"\n",
    "diario_texto = \"elPais\"\n",
    "counter_dictionary = {}\n",
    "\n",
    "def elPaisPalabrasLlaves(soup):\n",
    "    palabras_llaves = \"\"\n",
    "    separador = \", \"\n",
    "    # Hacemos un lista con las palabras llave\n",
    "    html_llaves = soup.find(\"ul\", class_=\"_df _ls\")\n",
    "    # Solo extraemos los enlaces\n",
    "    enlaces_llaves = html_llaves.find_all(\"a\")\n",
    "    # Sacamos el texto de los enlaces y y lo añadimos a la lista \n",
    "    for enlace in enlaces_llaves:\n",
    "        palabras_llaves = palabras_llaves + enlace.text + separador\n",
    "    # Quitamos el ultimo separador\n",
    "    palabras_llaves = palabras_llaves[:-(len(separador))]\n",
    "    return palabras_llaves\n",
    "\n",
    "def elPaisTexto(soup):\n",
    "    content = \"\"\n",
    "    # div con todo el texto\n",
    "    text = soup.find(\"div\", class_=\"a_c\")\n",
    "    # buscar todos los p\n",
    "    paragraphs = text.find_all(\"p\")\n",
    "    # Sacamos el contenido de p y lo añadimos a content\n",
    "    for paragraph in paragraphs:\n",
    "        content = content + paragraph.text + \"\\n\"\n",
    "    # print(content)\n",
    "    return content\n",
    "\n",
    "def elPaisTitulo(soup):\n",
    "    return soup.find(\"h1\", class_=\"a_t\").text\n",
    "\n",
    "def elPaisEntradilla(soup):\n",
    "    return soup.find(\"h2\", class_=\"a_st\").text\n",
    "\n",
    "def elPaisFecha(soup):\n",
    "    date_length = 10 # 2021-12-03T18:09:39.529Z\n",
    "    return soup.find(\"a\", id=\"article_date_p\")[\"data-date\"][:date_length]\n",
    "\n",
    "def guardarFichero(contenido, fecha, diario_texto, categoria):\n",
    "    # calcular numero (nnn)\n",
    "    counter = 1\n",
    "    if fecha in counter_dictionary[categoria]:\n",
    "        counter = counter_dictionary[categoria][fecha]\n",
    "        counter_dictionary[categoria][fecha] += 1\n",
    "    else:\n",
    "        counter_dictionary[categoria][fecha] = 2\n",
    "    \n",
    "    # /diario/categoria/categoria.fecha.nnn.txt\n",
    "    abs_path = os.getcwd()\n",
    "    rel_path =  diario_texto + \"/\" + categoria + \"/\"\n",
    "    nombre_fichero = categoria + \".\" + fecha + \".\" + str(counter).zfill(3) + \".txt\"\n",
    "    path = os.path.join(abs_path, rel_path, nombre_fichero)\n",
    "    try:\n",
    "        with open(path, 'x') as fichero:\n",
    "            fichero.write(contenido)\n",
    "        print (\"File \" + nombre_fichero + \" created!\")\n",
    "    except IOError:\n",
    "        print(\"File \" + nombre_fichero + \" already exists...\")\n",
    "\n",
    "\n",
    "def elPaisWebScraping(pagina, diario_texto, categoria):\n",
    "    soup = BeautifulSoup(pagina, 'html.parser')\n",
    "\n",
    "    palabras_llaves = elPaisPalabrasLlaves(soup)\n",
    "    titulo = elPaisTitulo(soup)\n",
    "    entradilla = elPaisEntradilla(soup)\n",
    "    texto = elPaisTexto(soup)\n",
    "    fecha = elPaisFecha(soup)\n",
    "\n",
    "    content = palabras_llaves + \"\\n\\n\" + titulo + \"\\n\\n\" + entradilla + \"\\n\\n\" + texto\n",
    "    guardarFichero(content,fecha,diario_texto, categoria)\n",
    "    return content\n",
    "\n",
    "def elPais(diario, diario_texto, categoria, url):\n",
    "    counter_dictionary[categoria] = {}\n",
    "    pagina = requests.get(url)\n",
    "    soup = BeautifulSoup(pagina.content, 'html.parser')\n",
    "\n",
    "    # sacar todas los enlaces de los articulos de la pagina categorial\n",
    "    enlaces = []\n",
    "    for article in soup.find_all(\"article\"):\n",
    "        enlaces.append(diario + article.find(\"a\")['href'])\n",
    "\n",
    "    # processar cada articulo\n",
    "    for enlace in enlaces: \n",
    "        articulo = requests.get(enlace)\n",
    "        try:\n",
    "            elPaisWebScraping(articulo.content, diario_texto, categoria)\n",
    "        except:\n",
    "            print(\"Error procesando articulo, saltando el articulo \" + enlace)\n",
    "\n",
    "\n",
    "\n",
    "# processar las categorias: salud, ciencia y tecnologia\n",
    "categoria = \"salud\"\n",
    "url = diario + \"/noticias/sanidad/\"\n",
    "elPais(diario, diario_texto, categoria, url)\n",
    "\n",
    "categoria = \"ciencia\"\n",
    "url = diario + \"/ciencia/\"\n",
    "elPais(diario, diario_texto, categoria, url)\n",
    "\n",
    "categoria = \"tecnologia\"\n",
    "url = diario + \"/tecnologia/\"\n",
    "elPais(diario, diario_texto, categoria, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a29d8f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
